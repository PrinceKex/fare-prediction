{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from colabcode import ColabCode\n",
    "# ColabCode(port=10000, password='Nelson123')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use 1% of data to speed up initial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/PrinceKex/fare-prediction/refs/heads/main/taxifare.csv'\n",
    "\n",
    "selected_cols = 'fare_amount,pickup_datetime,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count'.split(',')\n",
    "\n",
    "# define the data types\n",
    "dtypes = {\n",
    "  'fare_amount': 'float32',\n",
    "  'pickup_datetime': 'float32',\n",
    "  \"pickup_longitude\": 'float32',\n",
    "  'pickup_latitude': 'float32',\n",
    "  'dropoff_longitude': 'float32',\n",
    "  'dropoff_latitude': 'float32',\n",
    "  'passenger_count': 'uint8'\n",
    "}\n",
    "\n",
    "# define skip row function to randomize selection\n",
    "def skip_row(row_idx):\n",
    "  if row_idx == 0:\n",
    "    return False\n",
    "  return random.random() > 0.01\n",
    "random.seed(42)\n",
    "  \n",
    "\n",
    "\n",
    "df = pd.read_csv(url, usecols=selected_cols, parse_dates=['pickup_datetime'], dtype=dtypes)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explore the Dataset\n",
    "\n",
    "a. Basic info about the training set\n",
    "\n",
    "b. Basic info about the test set\n",
    "\n",
    "c. Exploratory data analysis and \n",
    " \n",
    "d. visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['pickup_datetime'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['pickup_datetime'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['pickup_datetime'].max(),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['pickup_datetime'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory data analylsis and Visualization\n",
    "Answer these questions about the data\n",
    "1. What is the busiest day of the week?\n",
    "2. What is the busiest time of the day?\n",
    "3. In which month are the fares the highest?\n",
    "4. Which pickup locations have the highest fares?\n",
    "5. Which drop location have the highest fares?\n",
    "6. What is the average ride distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Prepare Dataset for Training\n",
    "\n",
    "a. split the training and validation set\n",
    "\n",
    "b. fill/remove missing values\n",
    "\n",
    "c. extract inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_df, test_size=0.15, random_state=42)\n",
    "train_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.dropna()\n",
    "val_df = val_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns\n",
    "\n",
    "input_cols = ['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count']\n",
    "target_col = 'fare_amount'\n",
    "\n",
    "train_inputs = train_df[input_cols]\n",
    "train_targets = train_df[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_inputs = val_df[input_cols]\n",
    "val_targets = val_df[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_inputs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_targets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Train hardcoded and Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanRegressor: \n",
    "  def fit(self, inputs, targets):\n",
    "    self.mean = targets.mean()\n",
    "\n",
    "  def predict(self, inputs):\n",
    "    return np.full(inputs.shape[0], self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_model = MeanRegressor()\n",
    "mean_model.fit(train_inputs,  train_targets)\n",
    "mean_model.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = mean_model.predict(train_inputs)\n",
    "train_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = mean_model.predict(val_inputs)\n",
    "val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "def rmse( targets, preds):\n",
    "  return root_mean_squared_error(targets, preds)\n",
    "\n",
    "train_rmse = rmse(train_targets, train_preds)\n",
    "train_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_rmse = rmse(val_targets, val_preds)\n",
    "val_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Linear Regression Model on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(train_inputs, train_targets)\n",
    "\n",
    "train_preds = linear_model.predict(train_inputs)\n",
    "train_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse = rmse(train_targets, train_preds)\n",
    "train_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_rmse = rmse(val_targets, val_preds)\n",
    "val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = linear_model.predict(test_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the sample submission file and submit to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_submit(model, test_inputs, fname):\n",
    "  test_preds = model.predict(test_inputs)\n",
    "  sub_df = pd.read_csv('sample_submission.csv')\n",
    "  sub_df['fare_amount'] = test_preds\n",
    "  sub_df.to_csv(fname, index=None)\n",
    "  return sub_df\n",
    "\n",
    "predict_and_submit(linear_model, 'linear_model_submission')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering.\n",
    "\n",
    "a. extract part of the date\n",
    "\n",
    "b. remove outliers and invalid date\n",
    "\n",
    "c. add distance between pickups and drop\n",
    "\n",
    "d. add distance from landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dateparts(df, col):\n",
    "  df[col + '_year'] = df[col].dt.year\n",
    "  df[col + '_month'] = df[col].dt.month\n",
    "  df[col + '_day'] = df[col].dt.day\n",
    "  df[col + '_weekday'] = df[col].dt.weekday\n",
    "  df[col + '_hour'] = df[col].dt.hour\n",
    "\n",
    "add_dateparts(train_df, 'pickup_datetime')\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_dateparts(val_df, 'pickup_datetime')\n",
    "val_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_dateparts(test_df, 'pickup_datetime')\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Add distance between pickups and dropoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hoversine_np(lat1, lat2, len1, len2):\n",
    "  #calculate the distance between two points on the earth(specified as decimal degree)\n",
    "  # all args must be of equal length\n",
    "\n",
    "  len1, lat1, len2, lat2 = map(np.radians, [len1, lat1, len2, lat2])\n",
    "\n",
    "  dlen = len2 - len1\n",
    "  dlat = lat2 - lat1\n",
    "\n",
    "  a = np.sin(dlat/2)**2 * np.cos(lat1) * np.cos(lat2) * np.sin(dlen/2)**2\n",
    "\n",
    "  c = 2 * np.arcsin(np.sqrt(a))\n",
    "  km = 6367 * c\n",
    "  return km\n",
    "\n",
    "def add_trip_distance(df):\n",
    "  df['trip_distance'] = hoversine_np(df['pickup_longitude'], df['pickup_latitude'], df['dropoff_longitude'], df['dropoff_latitude'])\n",
    "\n",
    "\n",
    "add_trip_distance(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_trip_distance(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_trip_distance(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add distance from popular Landmarks. Airports, Times Square, Meuseum, World Trade Center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jfk_lonlat = -73.7781, 40.6413\n",
    "lga_lonlat = -73.8740, 40.7769\n",
    "ewr_lonlat = -74.1745, 40.6895\n",
    "met_lonlat = -74.9612, 40.7794\n",
    "wtc_lonlat = -74.0039, 40.7129\n",
    "\n",
    "def add_landmark_dropoff_distance(df, landmark_name, landmark_lonlat):\n",
    "  lon, lat = landmark_lonlat\n",
    "  df[landmark_name + 'drop_distance'] = hoversine_np(lon, lat, df['dropoff_longitude'], df['dropoff_latitude'])\n",
    "\n",
    "def add_landmarks(a_df):\n",
    "  landmarks = [('jfk', jfk_lonlat), ('lga', lga_lonlat), ('ewr', ewr_lonlat), ('wtc', wtc_lonlat)]\n",
    "  for name, lonlat in landmarks:\n",
    "    add_landmark_dropoff_distance(a_df, name, lonlat)\n",
    "\n",
    "add_landmarks(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_landmarks(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_landmarks(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Outliers and Invalid Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "  return df[(df['fare_amount'] >= 1.) & (df['fare_amount'] <= 500.) & (df['pickup_longitude'] >= -75) & (df['pickup_longitude'] <= -72) & (df['dropoff_longitude'] >= -75) & (df['dropoff_longitude'] <= -72) &  (df['pickup_latitude'] >= -75) & (df['pickup_latitude'] >= 40) & (df['pickup_latitude'] <= 42) & (df['dropoff_latitude'] >= 40) & (df['dropoff_latitude'] <= 42) & (df['passenger_count'] >= 1) & (df['passenger_count'] <= 6)] \n",
    "\n",
    "train_df = remove_outliers(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = remove_outliers(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = remove_outliers(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling and One Hot Encoding.\n",
    "Try scaling numeric columns to the (0, 1) range and encoding categorical columns using a one hot encoder.\n",
    "Not done because we are training a tree based model which is generally effecting without scaling and one hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Intermediate DataFrames\n",
    "save the dataframes in the parquet format so it can be reloaded when needed to continue the ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_parquet('train.parquet')\n",
    "test_df.to_parquet('test.parquet')\n",
    "val_df.to_parquet('val.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Evaluate different models\n",
    "1. Ridge Regression\n",
    "2. Random Forest\n",
    "3. Gradient Boosting\n",
    "4. Lasson\n",
    "5. SVM\n",
    "6. KNN\n",
    "7. Decision Tree Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = ['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count', 'pickup_datetime_year', 'pickup_datetime_month', 'pickup_datetime_day', 'pickup_datetime_weekday', 'pickup_datetime_hour', 'trip_distance', 'jfk_drop_distance', 'lga_drop_distance', 'ewr_drop_distance', 'met_drop_distance', 'wtc_drop_distance' ]\n",
    "target_col = 'fare_amount'\n",
    "\n",
    "train_inputs = train_df[input_cols]\n",
    "train_targets = train_df[target_col]\n",
    "\n",
    "val_inputs = val_df[input_cols]\n",
    "val_targets = val_df[target_col]\n",
    "\n",
    "test_inputs = test_df[input_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper function to evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model):\n",
    "  train_preds = model.predict(train_inputs)\n",
    "  train_rmse = rmse(train_targets, train_preds)\n",
    "  val_preds = model.predict(val_inputs)\n",
    "  val_rmse = rmse(val_targets, val_preds)\n",
    "  return train_rmse, val_rmse, train_preds, val_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "modelI = Ridge(random_state=42, alpha=0.9)\n",
    "\n",
    "modelI.fit(train_inputs, train_targets)\n",
    "\n",
    "evaluation(modelI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    " \n",
    "linear_modelII = RandomForestRegressor(random_state=42, n_jobs=-1, max_depth=10, n_estimators=100)\n",
    "\n",
    "modelII.fit(train_inputs, train_targets)\n",
    "\n",
    "evaluation(modelII)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "modelIII = XGBRegressor(max_depth=5, objective='reg:squared_error', n_estimators=200, random_state=42, n_jobs=-1)\n",
    "\n",
    "modelIII.fit(train_inputs, train_targets)\n",
    "\n",
    "evaluation(modelIII)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tunning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_params(ModelClass, **params):\n",
    "  model = ModelClass(**params).fit(train_inputs, train_targets)\n",
    "  train_rmse = rmse(model.predict(train_inputs), train_targets)\n",
    "  val_rmse = rmse(model.predict(val_inputs), train_targets)\n",
    "  return train_rmse, val_rmse\n",
    "\n",
    "def test_params_and_plot(ModelClass, param_name, param_value, **other_param):\n",
    "  train_errors, val_errors = [], []\n",
    "  for value in param_value:\n",
    "    params = dict[other_params]\n",
    "    params[param_name] = value\n",
    "    train_rmse, val_rmse = test_params(ModelClass, **params)\n",
    "    train_errors.append(train_rmse)\n",
    "    val_errors.append(val_rmse)\n",
    "\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  plt.title('Overfitting curve: ' + param_name)\n",
    "  plt.plot(param_value, train_errors, 'b-0')\n",
    "  plt.plot(param_value, val_errors, 'r-0')\n",
    "  plt.xlabel(param_name)\n",
    "  plt.ylabel('RMSE')\n",
    "  plt.legend(['Training', 'Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "  'random_state': 42,\n",
    "  'n_jobs': -1,\n",
    "  'objective': 'reg:squarederror'\n",
    "}\n",
    "\n",
    "test_params_and_plot(XGBRegressor, 'num_estimators', [100, 200, 400], **best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_final = XGBRegressor(objective='reg:squarederror', n_jobs=-1, random_state=42, n_estimators=500, learning_rate=0.08, subsample=0.7, colsample_bytree=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_final.fit(train_inputs, train_targets)\n",
    "evaluation(xgb_model_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
